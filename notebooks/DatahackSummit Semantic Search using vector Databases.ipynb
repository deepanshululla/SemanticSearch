{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e03b93",
   "metadata": {},
   "source": [
    "# Building Your Own Search Engine Using Vector Databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e8918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:40:26.682054Z",
     "start_time": "2023-07-05T08:40:26.542951Z"
    }
   },
   "source": [
    "![img](https://www.analyticsvidhya.com/datahack-summit-2023/wp-content/uploads/2023/07/s-won_searchengin.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef2022",
   "metadata": {},
   "source": [
    "## Agenda \n",
    "\n",
    "**Part 0: The Beginning**\n",
    "\n",
    "Welcome and Objectives : An introduction to the session's aims, a brief ice breaker activity, and setting the stage for the exploration ahead.\n",
    "\n",
    "Context Setting: A quick overview of AI, Search Engines, the current landscape, potential use-cases, benefits, and challenges. Highlight the significance of building an AI search engine with your data.\n",
    "\n",
    "**Part 1: Understanding the Basics**\n",
    "\n",
    "NLP and Search Engines: Explore the components like Natural Language Processing (NLP), Machine Learning algorithms, and their roles in crafting an efficient AI search engine. Topics include\n",
    "\n",
    "\n",
    "- What are vector embeddings?\n",
    "- Legacy vectorizing techniques like CountVectorizer, bag of words\n",
    "- Similarity measures and how do they work\n",
    "- LLM and Transformers\n",
    "\n",
    "Vector Databases\n",
    "  - An Exploration:\n",
    "  - Unveiling Vector Databases\n",
    "  - Understanding their workings\n",
    "  - Real-world use-cases\n",
    "  - A comparative analysis of available options\n",
    "\n",
    "**Part 2: Indexing**\n",
    "\n",
    "Splitting the data : Why is it required and different kinds of Data splitting. Also cover why splitting is context dependent (i.e depends on data)\n",
    "\n",
    "The next step is to insert the data into the database\n",
    "\n",
    "\n",
    "**Part 3: Searching**\n",
    "\n",
    "Performing Semantic Search on Indexed Data: Discuss and code the integration of NLP and Machine Learning algorithms into the search engine to comprehend, analyze, and generate precise search results from the given data. Employ Command line tools (or maybe a GUI) to execute the searches.\n",
    "\n",
    "Discuss Different Retrieval algorithms such as\n",
    "* MMR\n",
    "* LLM Aided Retrieval\n",
    "* Compression\n",
    "\n",
    "**Part 4: Question and answering**\n",
    "\n",
    "- Prompt Engineering and templates \n",
    "- Addressing a lot of windows and short context windows\n",
    "\n",
    "**Part 5: Chat**\n",
    "\n",
    "- Introducing memory\n",
    "- Followup conversations\n",
    "\n",
    "\n",
    "**Wrap-Up and Next Steps**\n",
    "\n",
    "Conclusion and Future Directions: Discuss steps to enhance the solution and where to go from here, providing a clear path for continued exploration and development.\n",
    "\n",
    "**References and Resources**\n",
    "\n",
    "\n",
    "## Checkbox\n",
    "\n",
    "\n",
    "### Demo Checkbox\n",
    "\n",
    "- [X]  **Part 1: Understanding the Basics**\n",
    "- [X]  **Part 2: Indexing**\n",
    "- [X]  **Part 3: Semantic search with vector db**\n",
    "- [X]  **Part 4: Question and answering**\n",
    "- [X]  **Part 5: Chat**\n",
    "\n",
    "\n",
    "### Theory material Checkbox\n",
    "\n",
    "- [X]  **Part 1: Understanding the Basics**\n",
    "- [X]  **Part 2: Indexing**\n",
    "- [X]  **Part 3: Semantic search with vector db**\n",
    "- [X]  **Part 4: Question and answering**\n",
    "- [X]  **Part 5: Chat**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64b282",
   "metadata": {},
   "source": [
    "## Libraries and Technologies we will use\n",
    "\n",
    "1) Pre Trained Large Language Model (LLM) like ChatGPT for vector Embedding\n",
    "2) Langchain for Supporting our model application\n",
    "3) Vector Database like Chroma\n",
    "4) Gradio\n",
    "\n",
    "\n",
    "## Generic Architecture\n",
    "\n",
    "![arch](https://ghost.hacksoft.io/content/images/2023/04/answering_questions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f6977",
   "metadata": {},
   "source": [
    "# Basics\n",
    "\n",
    "\n",
    "### Search Engines and Evolution With AI\n",
    "\n",
    "\n",
    "1. **Search Engines:** \n",
    "   - Search engines work as the librarians of the internet, scanning billions of pages to provide the most relevant results for your search queries. \n",
    "   - Traditional search engines operate primarily by scanning webpage text for matching keywords.\n",
    "\n",
    "2. **Limitations of Traditional Search Engines:**\n",
    "   - Traditional methods are limited, akin to finding a book in a library by only looking at the words on the covers without understanding the actual content of the book.\n",
    "\n",
    "3. **AI Revolution in Search Engines:**\n",
    "   - AI enables search engines to better understand the content and context of web pages and the user's intent.\n",
    "   - AI can distinguish the context in which a word is used, enhancing the relevance of search results.\n",
    "   - AI can personalize search results based on factors like previous searches and location.\n",
    "\n",
    "4. **Advanced Capabilities of AI-powered Search Engines:**\n",
    "   - AI-powered search engines can analyze and understand various data types including text, images, videos, and even voice commands.\n",
    "   - As a result, AI has transformed search engines from simple keyword-matching tools into sophisticated systems that understand and cater to the nuanced needs of users.\n",
    "   \n",
    "   \n",
    " ![search_engine](https://aeroadmin.com/articles/en/wp-content/uploads/2020/11/search-engine-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61debeee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T09:40:51.082165Z",
     "start_time": "2023-07-07T09:40:51.042885Z"
    }
   },
   "source": [
    "## Our Focus: Natural Language Search or Semantic Search\n",
    "\n",
    "Everyone knows we can use Chatgpt for searching and asking questions using Chatgpt\n",
    "\n",
    "\n",
    "![chatgpt](https://www.brookings.edu/wp-content/uploads/2023/01/shutterstock_2237655785.jpg)\n",
    "\n",
    "However it has certain limitations:\n",
    "1) It is trained on data before September 2021.\n",
    "2) We can't search and ask questions on our custom data\n",
    "\n",
    "But what if we want to augment LLM with our own data.\n",
    "\n",
    "### Retrieval augmented generation\n",
    " \n",
    "Retrieval-Augmented Generation (RAG) is a modern approach that can be used to enhance natural language search capabilities. It combines the best of retrieval-based and generative methods in NLP to answer questions or search queries.\n",
    "\n",
    "Here's a simplified explanation of how this might work:\n",
    "\n",
    "1. **Query Understanding:** Similar to traditional natural language search, the first step in RAG is understanding the user's query, which can involve Named Entity Recognition (NER), Part-of-Speech tagging (POS tagging), and other techniques to parse and interpret the query.\n",
    "\n",
    "2. **Document Retrieval:** RAG first retrieves a subset of documents from a larger corpus that are most relevant to the user's query. This retrieval step is based on a similarity measure, such as cosine similarity in the case of vectorized representations of text. The result is a shortlist of documents that are likely to contain the answer to the user's query.\n",
    "\n",
    "3. **Answer Generation:** Once the relevant documents are retrieved, a separate generative model takes the user's query and the retrieved documents as input and generates a response. The generative model can be a sequence-to-sequence model like a Transformer, which is trained to generate coherent and contextually appropriate text.\n",
    "\n",
    "\n",
    "![imng](https://cdn.thenewstack.io/media/2023/06/34141141-vector-db-llm-1024x665.png)\n",
    "\n",
    "RAG is a potent approach for natural language search, particularly for tasks that require understanding and generating language based on a large corpus of text, such as question answering or dialog systems. It combines the strengths of retrieval-based and generative models, allowing the model to access a vast amount of knowledge while generating detailed and contextually relevant responses.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e284397",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We can search on different kinds of data\n",
    "\n",
    "![search](https://redis.com/wp-content/uploads/2023/03/vector-similarity-diagram-1.svg?&auto=webp&quality=85,75&width=500)\n",
    "\n",
    "\n",
    "## Transforming Text In a Way Condusive for search\n",
    "\n",
    "Many Machine Learning algorithms and almost all Deep Learning Architectures are not capable of processing strings or plain text in their raw form. In a broad sense, they require numerical numbers as inputs to perform any sort of task, such as classification, regression, clustering, etc.\n",
    "\n",
    "Transforming text into vectors, known as vectorization, is a critical part of Natural Language Processing (NLP). This process allows us to convert human language into a format that machine learning algorithms can understand and work with. Here are a few common methods:\n",
    "\n",
    "\n",
    "\n",
    "**Bag of Words (BoW)**: This approach treats each document as an unordered bag or \"multiset\" of its words, disregarding grammar and word order but keeping the frequency of each word.\n",
    "\n",
    "![bow](https://miro.medium.com/v2/resize:fit:661/1*3K9GIOVLNu0cRvQap_KaRg.png)\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)**: This method reflects how important a word is to a document in a collection or corpus. It's often used as a weighting factor in text mining and information retrieval.\n",
    "\n",
    "![tfidf](https://lh6.googleusercontent.com/GTmNOZ5DkSorxEoATI93xvrBDCCOn0XAGDav8ybPJ0hIkqlk4nimcY9P8SNleZV1Cf8vnGVAlwawdZ5Fe8kPykKRZbHVUixjSPu1BJdd9DoAdgAVr5VMwhK2oSkXpyDFXunuON-l)\n",
    "\n",
    "**Word Embeddings**: These are dense vector representations in which similar words have similar vectors in the vector space. Word2Vec and GloVe are two popular methods of creating word embeddings. Word embeddings capture more nuanced semantic meanings and relationships between words compared to BoW and TF-IDF.\n",
    "\n",
    "![vectors](https://www.researchgate.net/publication/340825443/figure/fig6/AS:882927785238529@1587517796128/Word-embeddings-map-words-in-a-corpus-of-text-to-vector-space-Linear-combinations-of.png)\n",
    "\n",
    "\n",
    "**Word2Vec**\n",
    "- It is a shallow neural network model that generates word embeddings â€“ vectors that represent words in a high-dimensional space.\n",
    "- Word2Vec models, like Skip-gram and Continuous Bag of Words (CBOW), are trained to reconstruct the linguistic context of words.\n",
    "- Word2Vec can capture some semantic and syntactic relationships between words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1378b",
   "metadata": {},
   "source": [
    "## Transformers And LLMs\n",
    "\n",
    "### Limitations of Word2Vec\n",
    "1) Word2Vec does not consider the order of words (e.g., \"cat chases dog\" and \"dog chases cat\" are treated the same).\n",
    "2) Word2Vec generates a single vector representation for each word, regardless of the context (e.g., \"bank\" as a river bank and \"bank\" as a financial institution are treated the same).\n",
    "\n",
    "\n",
    "### Transitioning into Transformers\n",
    "\n",
    "To overcome these limitations, researchers developed new techniques and models that could understand context better.\n",
    "\n",
    "**Contextual Word Embeddings (ELMo, etc.)**: ELMo (Embeddings from Language Models) assigns embeddings to words based on their context, addressing the polysemy issue present in Word2Vec.\n",
    "\n",
    "**Transformers**: The Transformer model, introduced in the paper \"Attention is All You Need,\" revolutionized NLP. It's based on the idea of self-attention, allowing the model to weigh and understand the impact of each word on others in a sentence.\n",
    "\n",
    "The transformer model architecture underlies models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer). Unlike Word2Vec, these models provide context-aware representations by considering words in their specific context.\n",
    "\n",
    "**BERT** uses bidirectional training of Transformers, meaning it looks at the context from both left and right sides (words before and after the target word).\n",
    "\n",
    "**GPT** uses a transformer decoder and is trained in a unidirectional manner. It has been used extensively for tasks that require the generation of text.\n",
    "\n",
    "In summary, while we started with Word2Vec creating context-free embeddings, we've moved toward Transformer-based models that provide powerful context-dependent representations, leading to significant improvements in various NLP tasks.\n",
    "\n",
    "\n",
    "### How are LLMs related to transformers\n",
    "\n",
    "\n",
    "Large Language Models (LLMs) such as GPT-3 by OpenAI, BERT from Google, and RoBERTa from Facebook AI, all have their roots in the Transformer architecture and use vector embeddings as a fundamental part of their operation.\n",
    "\n",
    "* These models use the attention mechanism to generate context-aware word embeddings, superior to traditional embeddings generated by Word2Vec or GloVe.\n",
    "* They are trained on extensive amounts of text data and can understand and generate human-like text.\n",
    "* LLMs have achieved state-of-the-art performance on a wide range of NLP tasks, such as text classification, question answering, and named entity recognition\n",
    "\n",
    "\n",
    "![embedding](https://miro.medium.com/v2/resize:fit:1324/1*3XZgoCaZg1e9ySfXPTtfQg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3526d",
   "metadata": {},
   "source": [
    "## Vector Databases\n",
    "\n",
    "![db](https://miro.medium.com/v2/resize:fit:1400/1*VXrN-lBukxTqQVBrlS1lZg.png)\n",
    "\n",
    "Vector databases, also known as vector search engines or similarity search engines, play a crucial role in managing and leveraging high-dimensional vector data produced by Machine Learning (ML) models, including the Large Language Models (LLMs) in NLP tasks. Here's how they can be helpful:\n",
    "\n",
    "1. **Efficient Similarity Search**: Vector databases are designed to enable fast and efficient similarity search in high-dimensional vector spaces. They can help find the vectors that are most similar to a given vector, which is a common requirement in NLP tasks.\n",
    "\n",
    "2. **Scalable Storage and Retrieval**: Vector databases provide a scalable solution for storing and retrieving large amounts of high-dimensional vector data.\n",
    "\n",
    "3. **Integration with ML Workflows**: Vector databases can be easily integrated into ML workflows. They can serve as a bridge between the offline training of models and the online serving of results.\n",
    "\n",
    "4. **Use Cases in NLP**: In the context of NLP and language models, vector databases can be used to store and retrieve word or sentence embeddings. They can be used in tasks like semantic search (finding documents with similar meanings), recommendation systems (finding similar items or content), and more.\n",
    "\n",
    "5. **Real-time Applications**: With their ability to provide quick similarity search results, vector databases can support real-time applications such as chatbots, personalized recommendations, and more.\n",
    "\n",
    "In summary, vector databases help to store, manage, and retrieve the high-dimensional vector data produced by models like transformers, thus enabling the effective application of these models in various NLP tasks.\n",
    "\n",
    "\n",
    "![vectordb](https://cdn.sanity.io/images/vr8gru94/production/e88ebbacb848b09e477d11eedf4209d10ea4ac0a-1399x537.png)\n",
    "\n",
    "\n",
    "### When to use a vector database\n",
    "\n",
    "Using a vector database as opposed to a simple file for storing and retrieving vector data becomes essential in various situations. Here are some scenarios when you would want to opt for a vector database:\n",
    "\n",
    "1. **Large-scale Data**: If you're dealing with large-scale vector data, simply reading from and writing to files can be inefficient. A vector database is designed to handle such large amounts of data efficiently and quickly.\n",
    "\n",
    "2. **Efficient Searching**: If you need to perform similarity search queries on the vector data, a vector database provides sophisticated indexing mechanisms to allow efficient nearest-neighbor search, which would be highly inefficient with plain files.\n",
    "\n",
    "3. **Concurrency and Real-time Access**: If you need concurrent access to your vector data or if your application requires real-time data access, a vector database is a better fit as it supports these requirements out of the box. File-based systems, on the other hand, may struggle with concurrent accesses and real-time data retrieval.\n",
    "\n",
    "4. **Data Persistence and Reliability**: If you need your data to be reliably stored and persist across sessions or if you want built-in backup and restore capabilities, a vector database is a much safer bet than simple file storage.\n",
    "\n",
    "5. **Scalability and Distributed Computing**: Vector databases are built to work in distributed computing environments and can easily scale up or down based on the needs of your application. This is not typically possible with file-based storage without significant manual intervention.\n",
    "\n",
    "However, if your use case involves small-scale data, infrequent access, or does not require efficient search capabilities, a simple file-based storage might suffice. The choice between a vector database and a file really depends on the specifics of your use case and the scale and complexity of your data.\n",
    "\n",
    "\n",
    "### How does it work\n",
    "\n",
    "![vectordb](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83af13e6-8e84-4e37-84e9-244f3aa3e95b_2071x2470.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f591a5",
   "metadata": {},
   "source": [
    "### Commercially available Vector Databases\n",
    "\n",
    "1) Milvus\n",
    "2) Kinetica\n",
    "3) Chroma\n",
    "4) Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb888ac7",
   "metadata": {},
   "source": [
    "# Hands On Coding\n",
    "\n",
    "## Dependencies Installation and Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39fcfbe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:43.419508Z",
     "start_time": "2023-07-15T16:22:38.138987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (0.0.190)\n",
      "Requirement already satisfied: openai in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (0.27.8)\n",
      "Requirement already satisfied: chromadb in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (0.3.26)\n",
      "Requirement already satisfied: kaggle in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (1.5.15)\n",
      "Requirement already satisfied: sentence_transformers in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (2.13.1)\n",
      "Requirement already satisfied: gradio in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (3.36.1)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (2.0.17)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (1.25.0)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (1.10.10)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.3 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (2.0.3)\n",
      "Requirement already satisfied: hnswlib>=0.7 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (0.7.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (0.6.4)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (0.8.1)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (0.99.1)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (4.7.1)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (3.2.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (1.15.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (0.13.3)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from chromadb) (7.3.1)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from kaggle) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from kaggle) (8.0.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from kaggle) (2.0.3)\n",
      "Requirement already satisfied: bleach in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from kaggle) (6.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from sentence_transformers) (4.30.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from sentence_transformers) (0.16.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: aiofiles in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (23.1.0)\n",
      "Requirement already satisfied: altair>=4.2.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (5.0.1)\n",
      "Requirement already satisfied: ffmpy in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: gradio-client>=0.2.7 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (0.2.7)\n",
      "Requirement already satisfied: httpx in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (0.24.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: markupsafe in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (0.3.3)\n",
      "Requirement already satisfied: orjson in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (3.9.2)\n",
      "Requirement already satisfied: pillow in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: pydub in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: pygments>=2.12.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (2.15.1)\n",
      "Requirement already satisfied: python-multipart in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: semantic-version in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: websockets>=10.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from gradio) (11.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: pytz in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3)\n",
      "Requirement already satisfied: zstandard in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\n",
      "Requirement already satisfied: lz4 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb) (0.27.0)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.3)\n",
      "Requirement already satisfied: sympy in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from pandas>=1.3->chromadb) (2023.3)\n",
      "Requirement already satisfied: monotonic>=1.5 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: networkx in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.5.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from httpx->gradio) (0.17.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from matplotlib->gradio) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from matplotlib->gradio) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from matplotlib->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from matplotlib->gradio) (3.1.0)\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: uc-micro-py in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/anaconda3/envs/DatahackSummitSemanticSearch/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai chromadb kaggle sentence_transformers datasets gradio elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3134955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:43.429047Z",
     "start_time": "2023-07-15T16:22:43.422130Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "if not os.path.isfile(\"database.sqlite\"):\n",
    "    os.system(\"kaggle datasets download benhamner/nips-papersv\")\n",
    "    os.system(\"unzip -o nips-papers.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26bdddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:48.511721Z",
     "start_time": "2023-07-15T16:22:43.433371Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "import sqlite3\n",
    "from PyPDF2 import PdfReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b4e167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:51.451992Z",
     "start_time": "2023-07-15T16:22:48.514644Z"
    }
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"database.sqlite\")\n",
    "\n",
    "sql= \"\"\"WITH paper_author_list AS (\n",
    "    SELECT papers.id AS paper_id, Group_concat(authors.name) AS author_list\n",
    "    FROM papers\n",
    "    JOIN paper_authors ON papers.id = paper_authors.paper_id\n",
    "    JOIN authors ON paper_authors.author_id = authors.id\n",
    "    GROUP BY paper_id\n",
    ")\n",
    "SELECT papers.id AS paper_id, papers.year, papers.title, papers.abstract, papers.paper_text, paper_author_list.author_list AS authors\n",
    "FROM papers\n",
    "JOIN paper_author_list ON papers.id = paper_author_list.paper_id\n",
    "WHERE abstract NOT LIKE '%Abstract Missing%'\n",
    "\n",
    "\"\"\";\n",
    "\n",
    "papers_df = pd.read_sql_query(sql, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481493fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:51.484633Z",
     "start_time": "2023-07-15T16:22:51.454822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1861</td>\n",
       "      <td>2000</td>\n",
       "      <td>Algorithms for Non-negative Matrix Factorization</td>\n",
       "      <td>Non-negative matrix factorization (NMF) has pr...</td>\n",
       "      <td>Algorithms for Non-negative Matrix\\nFactorizat...</td>\n",
       "      <td>Daniel D. Lee,H. Sebastian Seung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975</td>\n",
       "      <td>2001</td>\n",
       "      <td>Characterizing Neural Gain Control using Spike...</td>\n",
       "      <td>Spike-triggered averaging techniques are effec...</td>\n",
       "      <td>Characterizing neural gain control using\\nspik...</td>\n",
       "      <td>Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3163</td>\n",
       "      <td>2007</td>\n",
       "      <td>Competition Adds Complexity</td>\n",
       "      <td>It is known that determinining whether a DEC-P...</td>\n",
       "      <td>Competition adds complexity\\n\\nJudy Goldsmith\\...</td>\n",
       "      <td>Judy Goldsmith,Martin Mundhenk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3164</td>\n",
       "      <td>2007</td>\n",
       "      <td>Efficient Principled Learning of Thin Junction...</td>\n",
       "      <td>We present the first truly polynomial algorith...</td>\n",
       "      <td>Efficient Principled Learning of Thin Junction...</td>\n",
       "      <td>Anton Chechetka,Carlos Guestrin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3167</td>\n",
       "      <td>2007</td>\n",
       "      <td>Regularized Boost for Semi-Supervised Learning</td>\n",
       "      <td>Semi-supervised inductive learning concerns ho...</td>\n",
       "      <td>Regularized Boost for Semi-Supervised Learning...</td>\n",
       "      <td>Ke Chen,Shihai Wang</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  year                                              title  \\\n",
       "0      1861  2000   Algorithms for Non-negative Matrix Factorization   \n",
       "1      1975  2001  Characterizing Neural Gain Control using Spike...   \n",
       "2      3163  2007                        Competition Adds Complexity   \n",
       "3      3164  2007  Efficient Principled Learning of Thin Junction...   \n",
       "4      3167  2007     Regularized Boost for Semi-Supervised Learning   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Non-negative matrix factorization (NMF) has pr...   \n",
       "1  Spike-triggered averaging techniques are effec...   \n",
       "2  It is known that determinining whether a DEC-P...   \n",
       "3  We present the first truly polynomial algorith...   \n",
       "4  Semi-supervised inductive learning concerns ho...   \n",
       "\n",
       "                                          paper_text  \\\n",
       "0  Algorithms for Non-negative Matrix\\nFactorizat...   \n",
       "1  Characterizing neural gain control using\\nspik...   \n",
       "2  Competition adds complexity\\n\\nJudy Goldsmith\\...   \n",
       "3  Efficient Principled Learning of Thin Junction...   \n",
       "4  Regularized Boost for Semi-Supervised Learning...   \n",
       "\n",
       "                                             authors  \n",
       "0                   Daniel D. Lee,H. Sebastian Seung  \n",
       "1  Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...  \n",
       "2                     Judy Goldsmith,Martin Mundhenk  \n",
       "3                    Anton Chechetka,Carlos Guestrin  \n",
       "4                                Ke Chen,Shihai Wang  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58712470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:51.498433Z",
     "start_time": "2023-07-15T16:22:51.488161Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_text(papers_df, column):\n",
    "    \n",
    "    # Load the regular expression library\n",
    "    import re\n",
    "    preprocessed_column = f\"{column}_processed\"\n",
    "\n",
    "    # Print the titles of the first rows \n",
    "    print(papers_df[column].head())\n",
    "\n",
    "    # remove punctuations\n",
    "    #papers_df[preprocessed_column] = papers_df[column].map(lambda x: re.sub('[,!?]', '', x))\n",
    "    \n",
    "     # remove carriage return and end of line\n",
    "    papers_df[preprocessed_column] = papers_df[column].map(lambda x: re.sub('[\\r\\n]', ' ', x))\n",
    "    \n",
    "     # remove double spaces\n",
    "    papers_df[preprocessed_column] = papers_df[preprocessed_column].map(lambda x: re.sub('  ', ' ', x))\n",
    "\n",
    "    \n",
    "    # remove para continuation\n",
    "    papers_df[preprocessed_column] = papers_df[preprocessed_column].map(lambda x: re.sub('- ', '', x))\n",
    "\n",
    "    # Convert the titles to lowercase\n",
    "    papers_df[preprocessed_column] = papers_df[preprocessed_column].map(lambda x: x.lower())\n",
    "    return papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1362e65f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:54.405452Z",
     "start_time": "2023-07-15T16:22:51.501566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Algorithms for Non-negative Matrix Factorization\n",
      "1    Characterizing Neural Gain Control using Spike...\n",
      "2                          Competition Adds Complexity\n",
      "3    Efficient Principled Learning of Thin Junction...\n",
      "4       Regularized Boost for Semi-Supervised Learning\n",
      "Name: title, dtype: object\n",
      "0    Non-negative matrix factorization (NMF) has pr...\n",
      "1    Spike-triggered averaging techniques are effec...\n",
      "2    It is known that determinining whether a DEC-P...\n",
      "3    We present the first truly polynomial algorith...\n",
      "4    Semi-supervised inductive learning concerns ho...\n",
      "Name: abstract, dtype: object\n",
      "0    Algorithms for Non-negative Matrix\\nFactorizat...\n",
      "1    Characterizing neural gain control using\\nspik...\n",
      "2    Competition adds complexity\\n\\nJudy Goldsmith\\...\n",
      "3    Efficient Principled Learning of Thin Junction...\n",
      "4    Regularized Boost for Semi-Supervised Learning...\n",
      "Name: paper_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "text_columns = [\"title\", \"abstract\", \"paper_text\"]\n",
    "for column in text_columns:\n",
    "    pre_process_text(papers_df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b7b97d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:54.416232Z",
     "start_time": "2023-07-15T16:22:54.407946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.abstract_processed[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d05f13e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:54.441436Z",
     "start_time": "2023-07-15T16:22:54.422196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>title_processed</th>\n",
       "      <th>abstract_processed</th>\n",
       "      <th>paper_text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1861</td>\n",
       "      <td>2000</td>\n",
       "      <td>Daniel D. Lee,H. Sebastian Seung</td>\n",
       "      <td>algorithms for non-negative matrix factorization</td>\n",
       "      <td>non-negative matrix factorization (nmf) has pr...</td>\n",
       "      <td>algorithms for non-negative matrix factorizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975</td>\n",
       "      <td>2001</td>\n",
       "      <td>Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...</td>\n",
       "      <td>characterizing neural gain control using spike...</td>\n",
       "      <td>spike-triggered averaging techniques are effec...</td>\n",
       "      <td>characterizing neural gain control using spike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3163</td>\n",
       "      <td>2007</td>\n",
       "      <td>Judy Goldsmith,Martin Mundhenk</td>\n",
       "      <td>competition adds complexity</td>\n",
       "      <td>it is known that determinining whether a dec-p...</td>\n",
       "      <td>competition adds complexity judy goldsmith dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3164</td>\n",
       "      <td>2007</td>\n",
       "      <td>Anton Chechetka,Carlos Guestrin</td>\n",
       "      <td>efficient principled learning of thin junction...</td>\n",
       "      <td>we present the first truly polynomial algorith...</td>\n",
       "      <td>efficient principled learning of thin junction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3167</td>\n",
       "      <td>2007</td>\n",
       "      <td>Ke Chen,Shihai Wang</td>\n",
       "      <td>regularized boost for semi-supervised learning</td>\n",
       "      <td>semi-supervised inductive learning concerns ho...</td>\n",
       "      <td>regularized boost for semi-supervised learning...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  year                                            authors  \\\n",
       "0      1861  2000                   Daniel D. Lee,H. Sebastian Seung   \n",
       "1      1975  2001  Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...   \n",
       "2      3163  2007                     Judy Goldsmith,Martin Mundhenk   \n",
       "3      3164  2007                    Anton Chechetka,Carlos Guestrin   \n",
       "4      3167  2007                                Ke Chen,Shihai Wang   \n",
       "\n",
       "                                     title_processed  \\\n",
       "0   algorithms for non-negative matrix factorization   \n",
       "1  characterizing neural gain control using spike...   \n",
       "2                        competition adds complexity   \n",
       "3  efficient principled learning of thin junction...   \n",
       "4     regularized boost for semi-supervised learning   \n",
       "\n",
       "                                  abstract_processed  \\\n",
       "0  non-negative matrix factorization (nmf) has pr...   \n",
       "1  spike-triggered averaging techniques are effec...   \n",
       "2  it is known that determinining whether a dec-p...   \n",
       "3  we present the first truly polynomial algorith...   \n",
       "4  semi-supervised inductive learning concerns ho...   \n",
       "\n",
       "                                paper_text_processed  \n",
       "0  algorithms for non-negative matrix factorizati...  \n",
       "1  characterizing neural gain control using spike...  \n",
       "2  competition adds complexity judy goldsmith dep...  \n",
       "3  efficient principled learning of thin junction...  \n",
       "4  regularized boost for semi-supervised learning...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df = papers_df.drop([\"title\", \"abstract\", \"paper_text\"], axis=1)\n",
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea936a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:54.449246Z",
     "start_time": "2023-07-15T16:22:54.443932Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_df = papers_df.rename(columns={'title_processed':'title', 'abstract_processed': 'abstract', 'paper_text_processed': 'paper_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdac785d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:54.465153Z",
     "start_time": "2023-07-15T16:22:54.452553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1861</td>\n",
       "      <td>2000</td>\n",
       "      <td>Daniel D. Lee,H. Sebastian Seung</td>\n",
       "      <td>algorithms for non-negative matrix factorization</td>\n",
       "      <td>non-negative matrix factorization (nmf) has pr...</td>\n",
       "      <td>algorithms for non-negative matrix factorizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975</td>\n",
       "      <td>2001</td>\n",
       "      <td>Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...</td>\n",
       "      <td>characterizing neural gain control using spike...</td>\n",
       "      <td>spike-triggered averaging techniques are effec...</td>\n",
       "      <td>characterizing neural gain control using spike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3163</td>\n",
       "      <td>2007</td>\n",
       "      <td>Judy Goldsmith,Martin Mundhenk</td>\n",
       "      <td>competition adds complexity</td>\n",
       "      <td>it is known that determinining whether a dec-p...</td>\n",
       "      <td>competition adds complexity judy goldsmith dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3164</td>\n",
       "      <td>2007</td>\n",
       "      <td>Anton Chechetka,Carlos Guestrin</td>\n",
       "      <td>efficient principled learning of thin junction...</td>\n",
       "      <td>we present the first truly polynomial algorith...</td>\n",
       "      <td>efficient principled learning of thin junction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3167</td>\n",
       "      <td>2007</td>\n",
       "      <td>Ke Chen,Shihai Wang</td>\n",
       "      <td>regularized boost for semi-supervised learning</td>\n",
       "      <td>semi-supervised inductive learning concerns ho...</td>\n",
       "      <td>regularized boost for semi-supervised learning...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  year                                            authors  \\\n",
       "0      1861  2000                   Daniel D. Lee,H. Sebastian Seung   \n",
       "1      1975  2001  Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...   \n",
       "2      3163  2007                     Judy Goldsmith,Martin Mundhenk   \n",
       "3      3164  2007                    Anton Chechetka,Carlos Guestrin   \n",
       "4      3167  2007                                Ke Chen,Shihai Wang   \n",
       "\n",
       "                                               title  \\\n",
       "0   algorithms for non-negative matrix factorization   \n",
       "1  characterizing neural gain control using spike...   \n",
       "2                        competition adds complexity   \n",
       "3  efficient principled learning of thin junction...   \n",
       "4     regularized boost for semi-supervised learning   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  non-negative matrix factorization (nmf) has pr...   \n",
       "1  spike-triggered averaging techniques are effec...   \n",
       "2  it is known that determinining whether a dec-p...   \n",
       "3  we present the first truly polynomial algorith...   \n",
       "4  semi-supervised inductive learning concerns ho...   \n",
       "\n",
       "                                          paper_text  \n",
       "0  algorithms for non-negative matrix factorizati...  \n",
       "1  characterizing neural gain control using spike...  \n",
       "2  competition adds complexity judy goldsmith dep...  \n",
       "3  efficient principled learning of thin junction...  \n",
       "4  regularized boost for semi-supervised learning...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861cf4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:54.471759Z",
     "start_time": "2023-07-15T16:22:54.467881Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_df = papers_df.drop([\"paper_text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9deb61f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:54.478432Z",
     "start_time": "2023-07-15T16:22:54.474043Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../secret/openai\") as f:\n",
    "    openai_secret = f.read().strip()\n",
    "    \n",
    "# PDF_FILE = \"../data/GenericEmailMarketting/merged_file.pdf\"\n",
    "\n",
    "# use import getpass instead\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_secret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "212a593a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:54.535413Z",
     "start_time": "2023-07-15T16:22:54.480768Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "llm.openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c21d7ef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:56.235955Z",
     "start_time": "2023-07-15T16:22:54.538621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "989397ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:57.198494Z",
     "start_time": "2023-07-15T16:22:56.239202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nThe current Prime Minister of the United Kingdom is Boris Johnson.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Who is the current prime minister of Britain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7921616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:57.206718Z",
     "start_time": "2023-07-15T16:22:57.202577Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "def get_openai_embedding(text):\n",
    "   text_rep = text.replace(\"\\n\", \" \")\n",
    "   return embeddings_model.embed_documents([text_rep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a49a512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:57.212435Z",
     "start_time": "2023-07-15T16:22:57.209228Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence1 = \"i like summer\"\n",
    "sentence2 = \"Brocholi on pizza is probably not a good idea\"\n",
    "sentence3 = \"I love the warm weather outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f66b98be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.342286Z",
     "start_time": "2023-07-15T16:22:57.215300Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding1 = embeddings_model.embed_query(sentence1)\n",
    "embedding2 = embeddings_model.embed_query(sentence2)\n",
    "embedding3 = embeddings_model.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "094e1686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.355866Z",
     "start_time": "2023-07-15T16:22:58.346164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7065412380498463\n",
      "0.7131438797298986\n",
      "0.8758719352747321\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(embedding1, embedding2))\n",
    "print(np.dot(embedding2, embedding3))\n",
    "print(np.dot(embedding1, embedding3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606365d1",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc5321d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.421686Z",
     "start_time": "2023-07-15T16:22:58.361515Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efbdd543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.443677Z",
     "start_time": "2023-07-15T16:22:58.424763Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(papers_df, page_content_column=\"abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d09697be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.795829Z",
     "start_time": "2023-07-15T16:22:58.446786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use lazy load for larger table, which won't read the full table into memory \n",
    "page = loader.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b063e7e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.803437Z",
     "start_time": "2023-07-15T16:22:58.798695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92c593b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.810145Z",
     "start_time": "2023-07-15T16:22:58.806086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': 1861,\n",
       " 'year': 2000,\n",
       " 'authors': 'Daniel D. Lee,H. Sebastian Seung',\n",
       " 'title': 'algorithms for non-negative matrix factorization'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "620c0386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.817102Z",
     "start_time": "2023-07-15T16:22:58.812247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.Document"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993d2d2",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c56d3",
   "metadata": {},
   "source": [
    "### Why do we need to split the data\n",
    "1) Chatgpt and LLM have limits\n",
    "\n",
    "![limits](https://miro.medium.com/v2/resize:fit:0/1*ihkkB2g7j9CMHBfvJtfLKw.png)\n",
    "\n",
    "2) To allow for efficient search for vector spaces\n",
    "\n",
    "![embeddings](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676309873/mirroredImages/pHPmMGEMYefk9jLeh/uvqfemlxskwrikptzeaq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0aa73477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.830600Z",
     "start_time": "2023-07-15T16:22:58.827760Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6dfe267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.835760Z",
     "start_time": "2023-07-15T16:22:58.832491Z"
    }
   },
   "outputs": [],
   "source": [
    "r_spltter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show.\n",
    "    chunk_size = 26,\n",
    "    chunk_overlap  = 4,\n",
    "    length_function = len,\n",
    "    #seperators=['\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 26,\n",
    "    chunk_overlap  = 4,\n",
    "    length_function = len,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90acef87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.845537Z",
     "start_time": "2023-07-15T16:22:58.839632Z"
    }
   },
   "outputs": [],
   "source": [
    "text_a2z = 'abcdefghijklmnopqrstuvwxyz'\n",
    "text_a2z_plus = 'abcdefghijklmnopqrstuvwxyz 12345678'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74f2a024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.862339Z",
     "start_time": "2023-07-15T16:22:58.848528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(text_a2z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25a2e5da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.872910Z",
     "start_time": "2023-07-15T16:22:58.865422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', '12345678']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(text_a2z_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "491d7b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.881874Z",
     "start_time": "2023-07-15T16:22:58.876229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text_a2z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e07b0f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.888371Z",
     "start_time": "2023-07-15T16:22:58.884251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz 12345678']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text_a2z_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1f09e",
   "metadata": {},
   "source": [
    "The issue is Character text splitter splits only on new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89eebdef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.894092Z",
     "start_time": "2023-07-15T16:22:58.890634Z"
    }
   },
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 26,\n",
    "    chunk_overlap  = 4,\n",
    "    length_function = len,\n",
    "    separator=\" \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "012f4002",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.905547Z",
     "start_time": "2023-07-15T16:22:58.896803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', '12345678']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text_a2z_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d14ea5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.915273Z",
     "start_time": "2023-07-15T16:22:58.910109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence. '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = page.page_content\n",
    "\n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7ba264c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.923479Z",
     "start_time": "2023-07-15T16:22:58.918040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix',\n",
       " 'factorization (nmf) has',\n",
       " 'has previously been shown',\n",
       " 'to  be a useful',\n",
       " 'decomposition for',\n",
       " 'for multivariate data.',\n",
       " 'two different multi',\n",
       " 'plicative algorithms for',\n",
       " 'for nmf are analyzed.',\n",
       " 'they differ only slightly',\n",
       " 'in  the multiplicative',\n",
       " 'factor used in the update',\n",
       " 'rules. one algorithm can',\n",
       " 'can be  shown to minimize',\n",
       " 'the conventional least',\n",
       " 'squares error while the',\n",
       " 'the other  minimizes the',\n",
       " 'the generalized',\n",
       " 'kullback-leibler',\n",
       " 'divergence. the monotonic',\n",
       " 'convergence of both',\n",
       " 'algorithms can be proven',\n",
       " 'using an auxiliary func',\n",
       " 'tion analogous to that',\n",
       " 'used for proving',\n",
       " 'convergence of the',\n",
       " 'the expectation',\n",
       " 'maximization algorithm.',\n",
       " 'the algorithms can also',\n",
       " 'be interpreted as diag',\n",
       " 'onally rescaled gradient',\n",
       " 'descent, where the',\n",
       " 'the rescaling factor is',\n",
       " 'is optimally  chosen to',\n",
       " 'to ensure convergence.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "844da99d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.936440Z",
     "start_time": "2023-07-15T16:22:58.929107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix',\n",
       " 'factorization (nmf) has',\n",
       " 'has previously been shown',\n",
       " 'to be a useful',\n",
       " 'decomposition for',\n",
       " 'for multivariate data. two',\n",
       " 'two different multi',\n",
       " 'plicative algorithms for',\n",
       " 'for nmf are analyzed. they',\n",
       " 'they differ only slightly',\n",
       " 'in the multiplicative',\n",
       " 'factor used in the update',\n",
       " 'rules. one algorithm can',\n",
       " 'can be shown to minimize',\n",
       " 'the conventional least',\n",
       " 'squares error while the',\n",
       " 'the other minimizes the',\n",
       " 'the generalized',\n",
       " 'kullback-leibler',\n",
       " 'divergence. the monotonic',\n",
       " 'convergence of both',\n",
       " 'both algorithms can be',\n",
       " 'be proven using an',\n",
       " 'an auxiliary func tion',\n",
       " 'tion analogous to that',\n",
       " 'that used for proving',\n",
       " 'convergence of the',\n",
       " 'the expectation',\n",
       " 'maximization algorithm.',\n",
       " 'the algorithms can also be',\n",
       " 'be interpreted as diag',\n",
       " 'diag onally rescaled',\n",
       " 'gradient descent, where',\n",
       " 'the rescaling factor is',\n",
       " 'is optimally chosen to',\n",
       " 'to ensure convergence.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6374d4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.948014Z",
     "start_time": "2023-07-15T16:22:58.940567Z"
    }
   },
   "outputs": [],
   "source": [
    "r_spltter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show.\n",
    "   \n",
    "    length_function = len,\n",
    "    separators=['\\n\\n', \"\\n\", \" \", \"\"],\n",
    "     chunk_size = 150,\n",
    "    chunk_overlap  = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5770388e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.957511Z",
     "start_time": "2023-07-15T16:22:58.951944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative',\n",
       " 'algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to',\n",
       " 'minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of',\n",
       " 'both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm.',\n",
       " 'the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure',\n",
       " 'convergence.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7177ee16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.966841Z",
     "start_time": "2023-07-15T16:22:58.962481Z"
    }
   },
   "outputs": [],
   "source": [
    "r_spltter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show.\n",
    "    length_function = len,\n",
    "    separators=['\\n\\n', \"\\n\", \" \",\"\\.\", \"\"],\n",
    "     chunk_size = 1000,\n",
    "    chunk_overlap  = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b438550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.975572Z",
     "start_time": "2023-07-15T16:22:58.970132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0b58bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.982220Z",
     "start_time": "2023-07-15T16:22:58.978402Z"
    }
   },
   "outputs": [],
   "source": [
    "# what if we want to split by sentences\n",
    "# regex with look behind\n",
    "sentence_spltter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show.\n",
    "    length_function = len,\n",
    "    separators=[\"(?<=\\.)\"],\n",
    "     chunk_size = 150,\n",
    "    chunk_overlap  = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b77495da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.990134Z",
     "start_time": "2023-07-15T16:22:58.985020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data.',\n",
       " 'two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules.',\n",
       " 'one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence.',\n",
       " ' the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm.',\n",
       " ' the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_spltter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34f6472a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:22:58.996265Z",
     "start_time": "2023-07-15T16:22:58.992878Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b8a069a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:06.750187Z",
     "start_time": "2023-07-15T16:22:58.998732Z"
    }
   },
   "outputs": [],
   "source": [
    "token_text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f7c5c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:06.764466Z",
     "start_time": "2023-07-15T16:23:06.752915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc',\n",
       " 'def',\n",
       " 'gh',\n",
       " 'ij',\n",
       " 'kl',\n",
       " 'mn',\n",
       " 'op',\n",
       " 'q',\n",
       " 'r',\n",
       " 'st',\n",
       " 'uv',\n",
       " 'w',\n",
       " 'xy',\n",
       " 'z']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text_splitter.split_text(text_a2z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef5a7247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:06.777704Z",
     "start_time": "2023-07-15T16:23:06.768131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non',\n",
       " '-',\n",
       " 'negative',\n",
       " ' matrix',\n",
       " ' factor',\n",
       " 'ization',\n",
       " ' (',\n",
       " 'nm',\n",
       " 'f',\n",
       " ')',\n",
       " ' has',\n",
       " ' previously',\n",
       " ' been',\n",
       " ' shown',\n",
       " ' to',\n",
       " ' ',\n",
       " ' be',\n",
       " ' a',\n",
       " ' useful',\n",
       " ' decom',\n",
       " 'position',\n",
       " ' for',\n",
       " ' mult',\n",
       " 'ivariate',\n",
       " ' data',\n",
       " '.',\n",
       " ' two',\n",
       " ' different',\n",
       " ' multi',\n",
       " ' pl',\n",
       " 'icative',\n",
       " ' algorithms',\n",
       " ' for',\n",
       " ' nm',\n",
       " 'f',\n",
       " ' are',\n",
       " ' analyzed',\n",
       " '.',\n",
       " ' they',\n",
       " ' differ',\n",
       " ' only',\n",
       " ' slightly',\n",
       " ' in',\n",
       " ' ',\n",
       " ' the',\n",
       " ' multipl',\n",
       " 'icative',\n",
       " ' factor',\n",
       " ' used',\n",
       " ' in',\n",
       " ' the',\n",
       " ' update',\n",
       " ' rules',\n",
       " '.',\n",
       " ' one',\n",
       " ' algorithm',\n",
       " ' can',\n",
       " ' be',\n",
       " ' ',\n",
       " ' shown',\n",
       " ' to',\n",
       " ' minimize',\n",
       " ' the',\n",
       " ' conventional',\n",
       " ' least',\n",
       " ' squares',\n",
       " ' error',\n",
       " ' while',\n",
       " ' the',\n",
       " ' other',\n",
       " ' ',\n",
       " ' minim',\n",
       " 'izes',\n",
       " ' the',\n",
       " ' generalized',\n",
       " ' k',\n",
       " 'ull',\n",
       " 'back',\n",
       " '-',\n",
       " 'le',\n",
       " 'ib',\n",
       " 'ler',\n",
       " ' divergence',\n",
       " '.',\n",
       " ' the',\n",
       " ' mon',\n",
       " 'ot',\n",
       " 'onic',\n",
       " ' ',\n",
       " ' convergence',\n",
       " ' of',\n",
       " ' both',\n",
       " ' algorithms',\n",
       " ' can',\n",
       " ' be',\n",
       " ' proven',\n",
       " ' using',\n",
       " ' an',\n",
       " ' auxiliary',\n",
       " ' func',\n",
       " ' tion',\n",
       " ' analogous',\n",
       " ' to',\n",
       " ' that',\n",
       " ' used',\n",
       " ' for',\n",
       " ' proving',\n",
       " ' convergence',\n",
       " ' of',\n",
       " ' the',\n",
       " ' expectation',\n",
       " ' maxim',\n",
       " 'ization',\n",
       " ' algorithm',\n",
       " '.',\n",
       " ' the',\n",
       " ' algorithms',\n",
       " ' can',\n",
       " ' also',\n",
       " ' be',\n",
       " ' interpreted',\n",
       " ' as',\n",
       " ' di',\n",
       " 'ag',\n",
       " ' on',\n",
       " 'ally',\n",
       " ' resc',\n",
       " 'aled',\n",
       " ' gradient',\n",
       " ' descent',\n",
       " ',',\n",
       " ' where',\n",
       " ' the',\n",
       " ' resc',\n",
       " 'aling',\n",
       " ' factor',\n",
       " ' is',\n",
       " ' optim',\n",
       " 'ally',\n",
       " ' ',\n",
       " ' chosen',\n",
       " ' to',\n",
       " ' ensure',\n",
       " ' convergence',\n",
       " '.',\n",
       " ' ']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text_splitter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abbbe69a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:07.155527Z",
     "start_time": "2023-07-15T16:23:06.780490Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b279b09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:07.161751Z",
     "start_time": "2023-07-15T16:23:07.157765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3921"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb678030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:07.169934Z",
     "start_time": "2023-07-15T16:23:07.164035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence. ', metadata={'paper_id': 1861, 'year': 2000, 'authors': 'Daniel D. Lee,H. Sebastian Seung', 'title': 'algorithms for non-negative matrix factorization'})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b7e1dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:07.175389Z",
     "start_time": "2023-07-15T16:23:07.172484Z"
    }
   },
   "outputs": [],
   "source": [
    "# For simplicity we will use sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f53ea8c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:07.630658Z",
     "start_time": "2023-07-15T16:23:07.177670Z"
    }
   },
   "outputs": [],
   "source": [
    "splits = sentence_spltter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6ee0239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:07.637883Z",
     "start_time": "2023-07-15T16:23:07.632794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data.', metadata={'paper_id': 1861, 'year': 2000, 'authors': 'Daniel D. Lee,H. Sebastian Seung', 'title': 'algorithms for non-negative matrix factorization'})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1d15f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:08.547999Z",
     "start_time": "2023-07-15T16:23:07.640038Z"
    }
   },
   "outputs": [],
   "source": [
    "random_splits = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345bde27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b511cfa7",
   "metadata": {},
   "source": [
    "## Indexing data in Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "caeab233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:08.554008Z",
     "start_time": "2023-07-15T16:23:08.550679Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9565318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:08.560944Z",
     "start_time": "2023-07-15T16:23:08.557196Z"
    }
   },
   "outputs": [],
   "source": [
    "persist_directory_random_split_gpt = 'data/chroma/random_split/gpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "65b81159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:38:54.312829Z",
     "start_time": "2023-07-15T17:38:53.977289Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf ./data/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "069b47e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:41:47.224284Z",
     "start_time": "2023-07-15T17:38:56.461979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 6.5 s, total: 1min 28s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectordb_gpt = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=persist_directory_random_split_gpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54ac24f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:50.877392Z",
     "start_time": "2023-07-15T16:23:50.873613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23526\n"
     ]
    }
   ],
   "source": [
    "print(vectordb_gpt._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abd9068c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:23:57.072940Z",
     "start_time": "2023-07-15T16:23:50.879697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fedb5e0e0bb40f6a5533b461c3fea4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectordb_gpt.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d22025b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T18:20:00.166788Z",
     "start_time": "2023-07-15T18:20:00.161654Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import ElasticVectorSearch\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticKnnSearch\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "elastic = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "index_name = \"test_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d7a96406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T18:21:25.862850Z",
     "start_time": "2023-07-15T18:21:17.346129Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/q4kkg14x6c9bg29754hkb_yr0000gn/T/ipykernel_45227/2402150112.py:1: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  elastic.delete_by_query(index=[index_name], body={\"query\": {\"match_all\": {}}})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'took': 8344, 'timed_out': False, 'total': 25143, 'deleted': 25143, 'batches': 26, 'version_conflicts': 0, 'noops': 0, 'retries': {'bulk': 0, 'search': 0}, 'throttled_millis': 0, 'requests_per_second': -1.0, 'throttled_until_millis': 0, 'failures': []})"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic.delete_by_query(index=[index_name], body={\"query\": {\"match_all\": {}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "60a0f4ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T18:25:21.554091Z",
     "start_time": "2023-07-15T18:21:34.866614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.8 s, sys: 10.9 s, total: 52.7 s\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "elasticdb_gpt = ElasticVectorSearch.from_documents(\n",
    "    splits,\n",
    "    embedding,\n",
    "    index_name=index_name,\n",
    "    elasticsearch_url=\"http://localhost:9200\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7ebfb775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T18:10:53.352492Z",
     "start_time": "2023-07-15T18:10:53.237245Z"
    }
   },
   "outputs": [],
   "source": [
    "elastic_vector_search = ElasticVectorSearch(\n",
    "            elasticsearch_url=\"http://localhost:9200\",\n",
    "            index_name=index_name,\n",
    "            embedding=embedding\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "6608d527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T18:11:03.289269Z",
     "start_time": "2023-07-15T18:11:03.282906Z"
    }
   },
   "outputs": [],
   "source": [
    "elastic_vector_search_knn = ElasticKnnSearch(\n",
    "\n",
    "            es_connection=elasticdb_gpt.client,\n",
    "            index_name=index_name,\n",
    "            embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca66e3",
   "metadata": {},
   "source": [
    "##  Search\n",
    "\n",
    "When a query comes in we first convert it to a vector and then compare the vector to the elements in the database to get n most similar results.\n",
    "\n",
    "**Similarity Search**\n",
    "1. Similarity Search aims to find the most similar items to a given query in a dataset.\n",
    "2. It uses metrics like cosine similarity, Jaccard similarity, or Euclidean distance to quantify similarity.\n",
    "3. The search results are ranked based on their similarity scores, and the top-K items are returned.\n",
    "4. Use-cases: When the user's intent is clear and specific, similarity search is efficient. It's commonly used in standard search engines, recommendation systems, or any scenario where the goal is to find items most similar to the query.\n",
    "\n",
    "**Maximal Marginal Relevance (MMR) Search**\n",
    "1. MMR aims to provide a diverse set of results that are relevant to the query.\n",
    "2. Along with quantifying similarity to the query, MMR also considers similarity between items in the results set to ensure diversity.\n",
    "3. It aims to maximize the relevance of the returned items to the query, but also minimize the similarity between the returned items.\n",
    "4. Use-cases: When user intent is ambiguous or when there are multiple relevant responses, MMR can provide a more diverse set of results. It's useful in news article recommendation (to avoid recommending too many similar articles) or in conversational AI (to provide diverse responses).\n",
    "\n",
    "The choice between similarity search and MMR depends on the specific use case and user needs. If the aim is to provide a diverse set of results, MMR would be more suitable. If the goal is to find items most similar to the query, a similarity search would be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93524545",
   "metadata": {},
   "source": [
    "### Keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5556765f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:51:48.767968Z",
     "start_time": "2023-07-15T17:51:43.947214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ce5f1e52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:51:50.379648Z",
     "start_time": "2023-07-15T17:51:50.311691Z"
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "45f36b49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:46:40.833411Z",
     "start_time": "2023-07-15T17:46:40.751049Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#keyword search\n",
    "resp = elasticdb_gpt.client.search(q=\"Natural language processing\", query={\"match_all\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d9c1f4e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:55:01.118331Z",
     "start_time": "2023-07-15T17:55:01.111923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2345 Hits:\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| text                                                                                                                                                                                                                                                    |   score |\n",
      "+=========================================================================================================================================================================================================================================================+=========+\n",
      "| cross language text classi?cation is an important learning task in natural language processing.                                                                                                                                                         | 18.6121 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| cross language text classi?cation is an important learning task in natural language processing.                                                                                                                                                         | 18.6121 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| Classically, tasks in natural language processing have been performed through rule-based and statistical methodologies.                                                                                                                                 | 16.4858 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| Classically, tasks in natural language processing have been performed through rule-based and statistical methodologies.                                                                                                                                 | 16.4858 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| In this paper, we present a study of the recent advancements which have helped bring Transfer Learning to NLP through the use of semi-supervised training.                                                                                              | 15.8581 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| We discuss cutting-edge methods and architectures such as BERT, GPT, ELMo, ULMFit among others.                                                                                                                                                         | 15.8581 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| However, owing to the vast nature of natural languages these methods do not generalise well and failed to learn the nuances of language.                                                                                                                | 15.8581 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| Thus machine learning algorithms such as Naive Bayes and decision trees coupled with traditional models such as Bag-of-Words and N-grams were used to usurp this problem.                                                                               | 15.8581 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| Eventually, with the advent of advanced recurrent neural network architectures such as the LSTM, we were able to achieve state-of-the-art performance in several natural language processing tasks such as text classification and machine translation. | 15.8581 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| We talk about how Transfer Learning has brought about the well-known ImageNet moment for NLP.                                                                                                                                                           | 15.8581 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "table = []\n",
    "for hit in resp['hits']['hits']:\n",
    "    table.append([hit['_source']['text'], hit['_score']])\n",
    "print(tabulate(table, headers=[\"text\", \"score\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d78c45",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ffc2a094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:55:18.242466Z",
     "start_time": "2023-07-15T17:55:17.159644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='users want natural language processing (nlp) systems to be both fast and accurate, but quality often comes at the cost of speed.', metadata={'paper_id': 4556, 'year': 2012, 'authors': 'Jiarong Jiang,Adam Teichert,Jason Eisner,Hal Daume', 'title': 'learned prioritization for trading off accuracy and speed'}),\n",
       " Document(page_content='cross language text classi?cation is an important learning task in natural language processing.', metadata={'paper_id': 5164, 'year': 2013, 'authors': 'Min Xiao,Yuhong Guo', 'title': 'a novel two-step method for cross language representation learning'}),\n",
       " Document(page_content='our framework is inspired by state-of-the-art smoothing techniques used in natural language processing (nlp).', metadata={'paper_id': 5880, 'year': 2015, 'authors': 'Pinar Yanardag,S.V.N. Vishwanathan', 'title': 'a structural smoothing framework for robust graph comparison'}),\n",
       " Document(page_content='teaching machines to read natural language documents remains an elusive challenge.', metadata={'paper_id': 5945, 'year': 2015, 'authors': 'Karl Moritz Hermann,Tomas Kocisky,Edward Grefenstette,Lasse Espeholt,Will Kay,Mustafa Suleyman,Phil Blunsom', 'title': 'teaching machines to read and comprehend'})]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.similarity_search(\"What is Natural language processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a4d83dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:55:43.470917Z",
     "start_time": "2023-07-15T17:55:43.143503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='users want natural language processing (nlp) systems to be both fast and accurate, but quality often comes at the cost of speed.', metadata={'paper_id': 4556, 'year': 2012, 'authors': 'Jiarong Jiang,Adam Teichert,Jason Eisner,Hal Daume', 'title': 'learned prioritization for trading off accuracy and speed'}),\n",
       "  0.284382700920105),\n",
       " (Document(page_content='cross language text classi?cation is an important learning task in natural language processing.', metadata={'paper_id': 5164, 'year': 2013, 'authors': 'Min Xiao,Yuhong Guo', 'title': 'a novel two-step method for cross language representation learning'}),\n",
       "  0.28937631845474243),\n",
       " (Document(page_content='our framework is inspired by state-of-the-art smoothing techniques used in natural language processing (nlp).', metadata={'paper_id': 5880, 'year': 2015, 'authors': 'Pinar Yanardag,S.V.N. Vishwanathan', 'title': 'a structural smoothing framework for robust graph comparison'}),\n",
       "  0.30891820788383484),\n",
       " (Document(page_content='teaching machines to read natural language documents remains an elusive challenge.', metadata={'paper_id': 5945, 'year': 2015, 'authors': 'Karl Moritz Hermann,Tomas Kocisky,Edward Grefenstette,Lasse Espeholt,Will Kay,Mustafa Suleyman,Phil Blunsom', 'title': 'teaching machines to read and comprehend'}),\n",
       "  0.3148042857646942)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.similarity_search_with_score(\"What is Natural language processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "470baeb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:35:57.841728Z",
     "start_time": "2023-07-15T17:35:57.477232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='when used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. when there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. we propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. we also develop a theory that motivates the algorithm.', metadata={'paper_id': 3686, 'year': 2009, 'authors': 'Yi-hao Kao,Benjamin V. Roy,Xiang Yan', 'title': 'directed regression'})]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.similarity_search(\"What is linear regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8ae07bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T18:04:18.725582Z",
     "start_time": "2023-07-15T18:04:15.728693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Classically, tasks in natural language processing have been performed through rule-based and statistical methodologies.', metadata={'title': 'Evolution of transfer learning in natural language processing', 'authors': 'Aditya Malte, Pratik Ratadiya', 'year': 2019, 'paper_id': 7302}),\n",
       "  1.8594966),\n",
       " (Document(page_content='users want natural language processing (nlp) systems to be both fast and accurate, but quality often comes at the cost of speed.', metadata={'paper_id': 4556, 'year': 2012, 'authors': 'Jiarong Jiang,Adam Teichert,Jason Eisner,Hal Daume', 'title': 'learned prioritization for trading off accuracy and speed'}),\n",
       "  1.8578087),\n",
       " (Document(page_content='cross language text classi?cation is an important learning task in natural language processing.', metadata={'paper_id': 5164, 'year': 2013, 'authors': 'Min Xiao,Yuhong Guo', 'title': 'a novel two-step method for cross language representation learning'}),\n",
       "  1.8553118),\n",
       " (Document(page_content='our framework is inspired by state-of-the-art smoothing techniques used in natural language processing (nlp).', metadata={'paper_id': 5880, 'year': 2015, 'authors': 'Pinar Yanardag,S.V.N. Vishwanathan', 'title': 'a structural smoothing framework for robust graph comparison'}),\n",
       "  1.8455409)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticdb_gpt.similarity_search_with_score(\"What is Natural language processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "71f8d296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T18:11:15.986021Z",
     "start_time": "2023-07-15T18:11:13.389208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='users want natural language processing (nlp) systems to be both fast and accurate, but quality often comes at the cost of speed.', metadata={'paper_id': 4556, 'year': 2012, 'authors': 'Jiarong Jiang,Adam Teichert,Jason Eisner,Hal Daume', 'title': 'learned prioritization for trading off accuracy and speed'}),\n",
       "  1.8570322),\n",
       " (Document(page_content='cross language text classi?cation is an important learning task in natural language processing.', metadata={'paper_id': 5164, 'year': 2013, 'authors': 'Min Xiao,Yuhong Guo', 'title': 'a novel two-step method for cross language representation learning'}),\n",
       "  1.8553118),\n",
       " (Document(page_content='our framework is inspired by state-of-the-art smoothing techniques used in natural language processing (nlp).', metadata={'paper_id': 5880, 'year': 2015, 'authors': 'Pinar Yanardag,S.V.N. Vishwanathan', 'title': 'a structural smoothing framework for robust graph comparison'}),\n",
       "  1.8455409),\n",
       " (Document(page_content='teaching machines to read natural language documents remains an elusive challenge.', metadata={'paper_id': 5945, 'year': 2015, 'authors': 'Karl Moritz Hermann,Tomas Kocisky,Edward Grefenstette,Lasse Espeholt,Will Kay,Mustafa Suleyman,Phil Blunsom', 'title': 'teaching machines to read and comprehend'}),\n",
       "  1.8425978)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_vector_search_knn.similarity_search_with_score(\"What is Natural language processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3099ef",
   "metadata": {},
   "source": [
    "### Maximum marginal relevance\n",
    "\n",
    "Maximum marginal relevance strives to achieve both relevance to the query and diversity among the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c764326d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:00.301404Z",
     "start_time": "2023-07-15T17:35:57.846174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='when used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. when there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. we propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. we also develop a theory that motivates the algorithm.', metadata={'paper_id': 3686, 'year': 2009, 'authors': 'Yi-hao Kao,Benjamin V. Roy,Xiang Yan', 'title': 'directed regression'}),\n",
       " Document(page_content='when used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. when there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. we propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. we also develop a theory that motivates the algorithm.', metadata={'paper_id': 3686, 'year': 2009, 'authors': 'Yi-hao Kao,Benjamin V. Roy,Xiang Yan', 'title': 'directed regression'}),\n",
       " Document(page_content='linear regression studies the problem of estimating a model parameter $\\\\beta^* \\\\in \\\\r^p$, from $n$ observations $\\\\{(y_i,x_i)\\\\}_{i=1}^n$ from linear model $y_i = \\\\langle \\\\x_i,\\\\beta^* \\\\rangle + \\\\epsilon_i$. we consider a significant generalization in which the relationship between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. this model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. we propose a novel spectral-based estimation procedure and show that we can recover $\\\\beta^*$ in settings (i.e., classes of link function $f$) where previous algorithms fail. in general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between $y_i$ and $\\\\langle x_i,\\\\beta^* \\\\rangle$. we also consider the high dimensional setting where $\\\\beta^*$ is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where $p \\\\gg n$. for a broad class of link functions between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.', metadata={'paper_id': 6013, 'year': 2015, 'authors': 'Xinyang Yi,Zhaoran Wang,Constantine Caramanis,Han Liu', 'title': 'optimal linear estimation under unknown nonlinear transform'}),\n",
       " Document(page_content='this article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. first, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.', metadata={'paper_id': 6751, 'year': 2017, 'authors': 'Daniel J. Hsu,Kevin Shi,Xiaorui Sun', 'title': 'linear regression without correspondence'})]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.max_marginal_relevance_search(\"What is linear regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a35827dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:00.826059Z",
     "start_time": "2023-07-15T17:36:00.304688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"a long-term goal of machine learning research is to build an intelligent dialog agent. most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). this kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. in this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. we study this setup in two domains: the babi dataset of (weston et al., 2015) and large-scale question answering from (dodge et al., 2015). we evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. in particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\", metadata={'paper_id': 6264, 'year': 2016, 'authors': 'Jason E. Weston', 'title': 'dialog-based language learning'}),\n",
       " Document(page_content=\"a long-term goal of machine learning research is to build an intelligent dialog agent. most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). this kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. in this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. we study this setup in two domains: the babi dataset of (weston et al., 2015) and large-scale question answering from (dodge et al., 2015). we evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. in particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\", metadata={'paper_id': 6264, 'year': 2016, 'authors': 'Jason E. Weston', 'title': 'dialog-based language learning'}),\n",
       " Document(page_content='teaching machines to read natural language documents remains an elusive challenge. machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.', metadata={'paper_id': 5945, 'year': 2015, 'authors': 'Karl Moritz Hermann,Tomas Kocisky,Edward Grefenstette,Lasse Espeholt,Will Kay,Mustafa Suleyman,Phil Blunsom', 'title': 'teaching machines to read and comprehend'}),\n",
       " Document(page_content='teaching machines to read natural language documents remains an elusive challenge. machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.', metadata={'paper_id': 5945, 'year': 2015, 'authors': 'Karl Moritz Hermann,Tomas Kocisky,Edward Grefenstette,Lasse Espeholt,Will Kay,Mustafa Suleyman,Phil Blunsom', 'title': 'teaching machines to read and comprehend'})]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.max_marginal_relevance_search(\"What is natural language processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8e95f320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:42:09.385236Z",
     "start_time": "2023-07-15T17:42:07.896672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.max_marginal_relevance_search(\"What is natural language processing\", filter={\"year\":1990})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc965eae",
   "metadata": {},
   "source": [
    "## Question and Answer\n",
    "\n",
    "\n",
    "When a query comes in we first convert it to a vector and then compare the vector to the elements in the database to get n most similar results. These results are then passed into prompt as a context for LLM to process them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "51923140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:01.845788Z",
     "start_time": "2023-07-15T17:36:01.830764Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "def query_db(db, users_question, llm, filter={}):\n",
    "  # define the prompt template\n",
    "  template = \"\"\"\n",
    "  Given the following context sections, answer the\n",
    "  question using only the given context. If you are unsure and the answer is not\n",
    "  explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "  Context sections:\n",
    "  {context}\n",
    "\n",
    "  Question:\n",
    "  {users_question}\n",
    "\n",
    "  Answer:\n",
    "  \"\"\"\n",
    "  prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "    # use our vector store to find similar text chunks\n",
    "  results = db.similarity_search(\n",
    "      query=users_question,\n",
    "      n_results=10,\n",
    "      filter=filter\n",
    "  )\n",
    "\n",
    "  # fill the prompt template\n",
    "  prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "  # ask the defined LLM\n",
    "  return llm(prompt_text)\n",
    "\n",
    "\n",
    "def query_db_relevance(db, users_question, llm, filter={}):\n",
    "  # define the prompt template\n",
    "  template = \"\"\"\n",
    "  Given the following context sections, answer the\n",
    "  question using only the given context. If you are unsure and the answer is not\n",
    "  explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "  Context sections:\n",
    "  {context}\n",
    "\n",
    "  Question:\n",
    "  {users_question}\n",
    "\n",
    "  Answer:\n",
    "  \"\"\"\n",
    "  prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "    # use our vector store to find similar text chunks\n",
    "  results = db.max_marginal_relevance_search(\n",
    "      query=users_question,\n",
    "      n_results=10,\n",
    "      filter=filter\n",
    "  )\n",
    "\n",
    "  # fill the prompt template\n",
    "  prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "  # ask the defined LLM\n",
    "  return llm(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "76d64bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:03.208945Z",
     "start_time": "2023-07-15T17:36:01.850243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sorry, I don't know how to help with that.\""
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is linear regression?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a8f5cadb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:06.493687Z",
     "start_time": "2023-07-15T17:36:03.213569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear regression is a method of estimating a model parameter from observations from a linear model, where the relationship between the covariates and the responses is unknown. It typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions.'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db_relevance(vectordb_gpt,\"What is linear regression?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "94573f65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:08.719894Z",
     "start_time": "2023-07-15T17:36:06.501658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Linear regression is a method of estimating a model parameter from observations from a linear model, typically involving estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions.'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(elasticdb_gpt,\"What is linear regression?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "787663b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:10.102507Z",
     "start_time": "2023-07-15T17:36:08.723622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sorry, I don't know how to help with that.\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is Natural language processing?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "87c172fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:13.710755Z",
     "start_time": "2023-07-15T17:36:10.106330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Natural language processing (NLP) is an important learning task in natural language processing which involves teaching machines to read and comprehend natural language documents. It is used to build systems that are both fast and accurate, but quality often comes at the cost of speed.'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(elasticdb_gpt,\"What is Natural language processing?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b8d6df13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:15.073170Z",
     "start_time": "2023-07-15T17:36:13.714783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sorry, I don't know how to help with that.\""
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is Natural language processing?\" , llm, {\"year\":1990})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "92850303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:16.401730Z",
     "start_time": "2023-07-15T17:36:15.077377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sorry, I don't know how to help with that.\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(elasticdb_gpt,\"What is Natural language processing?\" , llm, {\"year\":1990})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7c440512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:17.522864Z",
     "start_time": "2023-07-15T17:36:16.405260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sorry, I don't know how to help with that.\""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c004813d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:19.958935Z",
     "start_time": "2023-07-15T17:36:17.526264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I don't know how to help with that.\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db_relevance(vectordb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a56c",
   "metadata": {},
   "source": [
    "## Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8d871770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:19.985431Z",
     "start_time": "2023-07-15T17:36:19.964609Z"
    }
   },
   "outputs": [],
   "source": [
    "new_papers = [{\n",
    "    \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n",
    "    \"authors\": \"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\",\n",
    "    \"year\": 2018,\n",
    "    \"paper_id\": 7301,\n",
    "    \"abstract\": \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
    "\"\"\"\n",
    "},\n",
    "  {\n",
    "      \"title\" : \"Evolution of transfer learning in natural language processing\",\n",
    "       \"authors\": \"Aditya Malte, Pratik Ratadiya\",\n",
    "      \"year\": 2019,\n",
    "       \"paper_id\": 7302,\n",
    "      \"abstract\": \"\"\"In this paper, we present a study of the recent advancements which have helped bring Transfer Learning to NLP through the use of semi-supervised training. We discuss cutting-edge methods and architectures such as BERT, GPT, ELMo, ULMFit among others. Classically, tasks in natural language processing have been performed through rule-based and statistical methodologies. However, owing to the vast nature of natural languages these methods do not generalise well and failed to learn the nuances of language. Thus machine learning algorithms such as Naive Bayes and decision trees coupled with traditional models such as Bag-of-Words and N-grams were used to usurp this problem. Eventually, with the advent of advanced recurrent neural network architectures such as the LSTM, we were able to achieve state-of-the-art performance in several natural language processing tasks such as text classification and machine translation. We talk about how Transfer Learning has brought about the well-known ImageNet moment for NLP. Several advanced architectures such as the Transformer and its variants have allowed practitioners to leverage knowledge gained from unrelated task to drastically fasten convergence and provide better performance on the target task. This survey represents an effort at providing a succinct yet complete understanding of the recent advances in natural language processing using deep learning in with a special focus on detailing transfer learning and its potential advantages.\n",
    "\"\"\"    \n",
    "  },\n",
    "  {\n",
    "       \"title\" : \"BERTQA -- Attention on Steroids\",\n",
    "       \"authors\": \"Ankit Chadha, Rewa Sood\",\n",
    "      \"year\": 2019,\n",
    "        \"paper_id\": 7303,\n",
    "      \"abstract\": \"\"\"In this work, we extend the Bidirectional Encoder Representations from Transformers (BERT) with an emphasis on directed coattention to obtain an improved F1 performance on the SQUAD2.0 dataset. The Transformer architecture on which BERT is based places hierarchical global attention on the concatenation of the context and query. Our additions to the BERT architecture augment this attention with a more focused context to query (C2Q) and query to context (Q2C) attention via a set of modified Transformer encoder units. In addition, we explore adding convolution-based feature extraction within the coattention architecture to add localized information to self-attention. We found that coattention significantly improves the no answer F1 by 4 points in the base and 1 point in the large architecture. After adding skip connections the no answer F1 improved further without causing an additional loss in has answer F1. The addition of localized feature extraction added to attention produced an overall dev F1 of 77.03 in the base architecture. We applied our findings to the large BERT model which contains twice as many layers and further used our own augmented version of the SQUAD 2.0 dataset created by back translation, which we have named SQUAD 2.Q. Finally, we performed hyperparameter tuning and ensembled our best models for a final F1/EM of 82.317/79.442 (Attention on Steroids, PCE Test Leaderboard).\n",
    "\"\"\"             \n",
    "  },\n",
    "    {\n",
    "        \"title\": \"BERT: A Review of Applications in Natural Language Processing and Understanding\",\n",
    "        \"authors\": \"Mikhail Koroteev\",\n",
    "        \"year\": 2021,\n",
    "        \"paper_id\": 7304,\n",
    "        \"abstract\": \"In this review, we describe the application of one of the most popular deep learning-based language models - BERT. The paper describes the mechanism of operation of this model, the main areas of its application to the tasks of text analytics, comparisons with similar models in each task, as well as a description of some proprietary models. In preparing this review, the data of several dozen original scientific articles published over the past few years, which attracted the most attention in the scientific community, were systematized. This survey will be useful to all students and researchers who want to get acquainted with the latest advances in the field of natural language text analysis.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0981f3de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:20.012464Z",
     "start_time": "2023-07-15T17:36:19.992192Z"
    }
   },
   "outputs": [],
   "source": [
    "new_paper_df = pd.DataFrame(new_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "be2f0e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:20.049549Z",
     "start_time": "2023-07-15T17:36:20.015584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kris...</td>\n",
       "      <td>2018</td>\n",
       "      <td>7301</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evolution of transfer learning in natural lang...</td>\n",
       "      <td>Aditya Malte, Pratik Ratadiya</td>\n",
       "      <td>2019</td>\n",
       "      <td>7302</td>\n",
       "      <td>In this paper, we present a study of the recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BERTQA -- Attention on Steroids</td>\n",
       "      <td>Ankit Chadha, Rewa Sood</td>\n",
       "      <td>2019</td>\n",
       "      <td>7303</td>\n",
       "      <td>In this work, we extend the Bidirectional Enco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BERT: A Review of Applications in Natural Lang...</td>\n",
       "      <td>Mikhail Koroteev</td>\n",
       "      <td>2021</td>\n",
       "      <td>7304</td>\n",
       "      <td>In this review, we describe the application of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "1  Evolution of transfer learning in natural lang...   \n",
       "2                    BERTQA -- Attention on Steroids   \n",
       "3  BERT: A Review of Applications in Natural Lang...   \n",
       "\n",
       "                                             authors  year  paper_id  \\\n",
       "0  Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kris...  2018      7301   \n",
       "1                      Aditya Malte, Pratik Ratadiya  2019      7302   \n",
       "2                            Ankit Chadha, Rewa Sood  2019      7303   \n",
       "3                                   Mikhail Koroteev  2021      7304   \n",
       "\n",
       "                                            abstract  \n",
       "0  We introduce a new language representation mod...  \n",
       "1  In this paper, we present a study of the recen...  \n",
       "2  In this work, we extend the Bidirectional Enco...  \n",
       "3  In this review, we describe the application of...  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1dce5812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:20.064329Z",
     "start_time": "2023-07-15T17:36:20.052256Z"
    }
   },
   "outputs": [],
   "source": [
    "loader_new = DataFrameLoader(new_paper_df, page_content_column=\"abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f0a69bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:20.081975Z",
     "start_time": "2023-07-15T17:36:20.067083Z"
    }
   },
   "outputs": [],
   "source": [
    "new_splits = sentence_spltter.split_documents(loader_new.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f2cad741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:23.934738Z",
     "start_time": "2023-07-15T17:36:20.085107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1c0d22a2-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d24c8-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2518-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d254a-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2572-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d259a-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d25cc-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d25f4-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d261c-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2644-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d266c-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d268a-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d26b2-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d26da-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d270c-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d272a-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2752-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d277a-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d27a2-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d27ca-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d27f2-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d281a-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2838-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2860-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2888-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d28b0-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d28d8-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2900-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2928-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d2950-2336-11ee-8d97-82ad35693800',\n",
       " '1c0d296e-2336-11ee-8d97-82ad35693800']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.add_documents(new_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3ee1e8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:25.735759Z",
     "start_time": "2023-07-15T17:36:23.939042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['592b159b-46e2-44a7-956f-eb6b075e333b',\n",
       " 'd601b398-af58-41f9-aa0d-e4885274667b',\n",
       " 'c0f99f2b-8911-4929-aa4d-da1344d8595f',\n",
       " 'e47d86d7-b529-4c5f-b51a-3c288391c9a5',\n",
       " '4cca50ab-f515-4b39-857b-c7a932987221',\n",
       " '7c6b5aca-1485-4268-a498-48c9dda7bbd6',\n",
       " 'be47a1cf-7855-4842-9a2c-10abd5d1f889',\n",
       " '52215b6d-ba56-4ba9-a082-fa47f09ab066',\n",
       " '9eb4aa2a-9504-45f3-a300-ec5690e68357',\n",
       " 'f6a958fe-7138-4b83-9b76-c3705b5c0ef5',\n",
       " 'ed164754-5a62-491c-b30d-c3aa411b91a2',\n",
       " '36c7e843-8e01-4dfb-8ffa-6621fe3d9450',\n",
       " '58f73c9b-34a7-4440-be86-546d785f28b8',\n",
       " '3571b58f-d433-49be-ac4c-90e70efcc499',\n",
       " 'd07ccdef-600f-4341-a047-dff561dea519',\n",
       " '773d6339-a3f2-4725-9547-c08ebb8e126c',\n",
       " 'fed326ac-f206-4247-b222-b0a308b8436b',\n",
       " '614ede5a-3acd-400b-a2aa-d72849d126b2',\n",
       " '43361fc1-c2ed-43b4-89fb-a6537e1c8c86',\n",
       " 'ee21e7c9-4357-4b8a-b470-0ea09e4cbbbe',\n",
       " '1c16cc6a-145c-4746-b47e-99ec48ee36cd',\n",
       " '74368317-04df-47f4-bf28-fc00fd53aec2',\n",
       " 'd80adf31-0f63-45e5-9d5e-1e4b78b56a2f',\n",
       " 'e3a8a0c8-9972-4f1d-ba2a-9b664c5e1c41',\n",
       " '92fc48a9-bb3c-4066-a6d6-d5bf8260ae3e',\n",
       " '187ed0c3-1d1b-4933-94f2-5ea9f7fce7a6',\n",
       " '051b33c9-8b32-4e76-9b7f-8a8747c91483',\n",
       " '3ed95e0b-5ff0-4958-8ed5-861a64d90259',\n",
       " '42e481d5-d4ac-4f08-a158-80174355f8e3',\n",
       " '82f36f8f-bee6-4d20-b3cd-214fab60b5ce',\n",
       " '6f92ef3c-c696-4039-b059-3c4a712cc514']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticdb_gpt.add_documents(new_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ffced776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:27.067854Z",
     "start_time": "2023-07-15T17:36:25.738964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BERT is a new language representation model called Bidirectional Encoder Representations from Transformers.'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1cd77b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:29.789171Z",
     "start_time": "2023-07-15T17:36:27.070889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BERT is a new language representation model called Bidirectional Encoder Representations from Transformers.'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db_relevance(vectordb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "68327365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:32.321321Z",
     "start_time": "2023-07-15T17:36:29.795189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BERT is a new language representation model called Bidirectional Encoder Representations from Transformers.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(elasticdb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "40b596b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:33.611485Z",
     "start_time": "2023-07-15T17:36:32.323830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I don't know how to help with that.\""
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db_relevance(vectordb_gpt,\"How is bert an improvement?\" , llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b785909",
   "metadata": {},
   "source": [
    "## Chatting with your data: Final Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4e8daff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:33.659860Z",
     "start_time": "2023-07-15T17:36:33.637344Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fbccf78d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:33.689574Z",
     "start_time": "2023-07-15T17:36:33.662639Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "retriever=vectordb_gpt.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e797ac09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:35.462827Z",
     "start_time": "2023-07-15T17:36:33.692219Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"What is Natural language processing?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2a6ffbf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:35.477463Z",
     "start_time": "2023-07-15T17:36:35.466937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Natural language processing is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fa5b9862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:38.606228Z",
     "start_time": "2023-07-15T17:36:35.481795Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"What are its real life applications?\"\n",
    "result2 = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "38543a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:38.626078Z",
     "start_time": "2023-07-15T17:36:38.610084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Natural language processing can be used for tasks such as text classification, machine translation, sentiment analysis, and question answering.'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " result2[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c0d9a92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:38.647472Z",
     "start_time": "2023-07-15T17:36:38.630890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "current_date = datetime.datetime.now().date()\n",
    "if current_date < datetime.date(2023, 9, 2):\n",
    "    llm_name = \"gpt-3.5-turbo-0301\"\n",
    "else:\n",
    "    llm_name = \"gpt-3.5-turbo\"\n",
    "print(llm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ee95cfc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:38.667337Z",
     "start_time": "2023-07-15T17:36:38.652040Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def load_db(file, chain_type, k):\n",
    "    # create vector database from data\n",
    "    db = vectordb_gpt\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "#         return_generated_question=True,\n",
    "    )\n",
    "    return qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "332c134b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:38.689408Z",
     "start_time": "2023-07-15T17:36:38.671469Z"
    }
   },
   "outputs": [],
   "source": [
    "import gradio\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history_ui\",\n",
    "    return_messages=True\n",
    ")\n",
    "retriever=vectordb_gpt.as_retriever()\n",
    "    \n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=\"stuff\", \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "def chat(message, history=None):\n",
    "    history = history or []\n",
    "    response = qa({\"question\": message, \"chat_history\":history})['answer']\n",
    "    print(message, response, history)\n",
    "    history.append((message, response))\n",
    "    return history, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3d71bb01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T17:36:52.370636Z",
     "start_time": "2023-07-15T17:36:38.692418Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/q4kkg14x6c9bg29754hkb_yr0000gn/T/ipykernel_45227/3380930186.py:2: GradioDeprecationWarning: The 'color_map' parameter has been deprecated.\n",
      "  chatbot = gradio.Chatbot(color_map=(\"green\", \"gray\"))\n",
      "/var/folders/3c/q4kkg14x6c9bg29754hkb_yr0000gn/T/ipykernel_45227/3380930186.py:3: GradioDeprecationWarning: `allow_screenshot` parameter is deprecated, and it has no effect\n",
      "  interface = gradio.Interface(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://df2f31e9b157b3103d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://df2f31e9b157b3103d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collection.load()\n",
    "chatbot = gradio.Chatbot(color_map=(\"green\", \"gray\"))\n",
    "interface = gradio.Interface(\n",
    "    chat,\n",
    "    [\"text\", \"state\"],\n",
    "    [chatbot, \"state\"],\n",
    "    allow_screenshot=False,\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "interface.launch(inline=True, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4cc640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DatahackSummitSemanticSearch",
   "language": "python",
   "name": "datahacksummitsemanticsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
